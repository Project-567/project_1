{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "discount_factor = 0.99 # small prefer immediate reward, large prefer future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# # for equal probability amongst all actions, divide everything by the number of actions\n",
    "# policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# # policy at state 0 = [0, 0]\n",
    "# # returns a probability for each action given state\n",
    "# policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 8.88545230282034e-07 iterations: 2609'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "        # save data for plot\n",
    "        delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201.9998, 204.0402, 201.9998, 199.0402, 197.0498],\n",
       "       [199.9798, 201.9998, 199.9798, 197.98  , 196.0002],\n",
       "       [197.98  , 199.9798, 197.98  , 196.0002, 194.0402],\n",
       "       [196.0002, 197.98  , 196.0002, 194.0402, 192.0998],\n",
       "       [194.0402, 196.0002, 194.0402, 192.0998, 190.1788]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list)\n",
    "plt.title('Policy Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Policy-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2     3     4\n",
       "0  right  up  left    up  left\n",
       "1     up  up    up  left  left\n",
       "2     up  up    up    up    up\n",
       "3     up  up    up    up    up\n",
       "4     up  up    up    up    up"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.99\n",
    "actions = [[1, 0], [0, 1], [-1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "#         if -1 in self.pos_check or self.size in self.pos_check: \n",
    "\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "    \n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a random initial state\n",
    "grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all possible states\n",
    "grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 9.854787648144736e-07 iterations: 590'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy evaluation\n",
    "    # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "    # to calculate the value of each action.\n",
    "    # Replace the value map with the calculated value.\n",
    "\n",
    "theta = 0.0001\n",
    "iterations = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    delta = 0\n",
    "    iterations+=1\n",
    "    valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "    # start with the first state in the state list\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "        value = 0\n",
    "        \n",
    "        # perform 4 actions per state and add the rewards (value)\n",
    "        for action_number, action in enumerate(actions):\n",
    "            \n",
    "            # get next position and reward\n",
    "            new_position = grid.p_transition(state, action)\n",
    "            reward = grid.reward(state, action)\n",
    "            \n",
    "            # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "            value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "        # replace the value in valueMap with the value\n",
    "        valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "        # calculate delta\n",
    "        delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "        clear_output(wait=True)\n",
    "        display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "        # overwrite the original value map\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "    # stop when change in value function falls below a given threshold\n",
    "    if delta < theta:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.21746899,  7.12429912,  4.30304947,  4.75253267,  1.52444191],\n",
       "       [ 1.46083571,  2.72465172,  2.21627535,  1.75650401,  0.37814393],\n",
       "       [-0.49049183,  0.20728173,  0.17044079, -0.24996795, -1.12113646],\n",
       "       [-2.14931268, -1.56710063, -1.48494137, -1.81578172, -2.52678468],\n",
       "       [-3.46708837, -2.90474932, -2.78732345, -3.07479967, -3.73542904]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display value function\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 1. 0. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [1. 0. 0. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 0. 1.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [1. 0. 0. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 0. 1.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 0. 1.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n",
      "before policy:  [0.25 0.25 0.25 0.25]\n",
      "after policy:  [0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# policy improvement\n",
    "\n",
    "for state_number, state in enumerate(grid.states):\n",
    "\n",
    "    # The best action we would take under the current policy\n",
    "    chosen_a = np.argmax(policy[state_number])\n",
    "#     print(\"states: \", state)\n",
    "#     print(\"policy: \", policy[state_number])\n",
    "#     print(\"chosen a: \", chosen_a)\n",
    "\n",
    "    # calculate the value of all actions in a given state\n",
    "    # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "    action_values = calculate_action_value(state, grid.valueMap)\n",
    "#     print(\"action values: \", action_values)\n",
    "    \n",
    "    # take the action with the highest value\n",
    "    best_a = np.argmax(action_values)\n",
    "\n",
    "    # Greedily update the policy\n",
    "    if chosen_a != best_a:\n",
    "        policy_stable = False\n",
    "\n",
    "    # update the policy with the best action\n",
    "    print(\"before policy: \", policy[state_number])\n",
    "    policy[state_number] = np.eye(action_count)[best_a]\n",
    "    print(\"after policy: \", policy[state_number])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#     # The best action we would take under the current policy\n",
    "#     chosen_a = np.argmax(policy[state_number])\n",
    "#     print(\"states: \", state)\n",
    "#     print(\"policy: \", policy[state_number])\n",
    "#     print(\"chosen a: \", chosen_a)\n",
    "\n",
    "#     action_values = calculate_action_value(state, grid.valueMap)\n",
    "#     print(\"action values: \", action_values)\n",
    "#     best_a = np.argmax(action_values)\n",
    "    \n",
    "#     # Greedily update the policy\n",
    "#     if chosen_a != best_a:\n",
    "#         policy_stable = False\n",
    "        \n",
    "#     print(\"before policy: \", policy[state_number])\n",
    "#     policy[state_number] = np.eye(action_count)[best_a]\n",
    "#     print(\"after policy: \", policy[state_number])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get transition: starting in state [0, 0] with action [0, 1], the new state is [0, 1]\n",
    "state = [0, 0]\n",
    "action = [0, 1] # move right\n",
    "grid = Gridworld(5)\n",
    "new_position = grid.p_transition(state, action)\n",
    "new_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to solve a system of equations\n",
    "a = np.array([[0.5, 0, 0], [-0.5, 1, 0], [0, -0.5, 1]])\n",
    "b = np.array([0,0,1])\n",
    "x = np.linalg.solve(a,b)\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

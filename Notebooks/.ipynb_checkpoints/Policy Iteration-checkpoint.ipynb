{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "discount_factor = 0.99 # small prefer immediate reward, large prefer future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 9.438184633836499e-07 iterations: 2693'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "            # save data for plot\n",
    "            delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201.9998, 204.0402, 201.9998, 199.0402, 197.0498],\n",
       "       [199.9798, 201.9998, 199.9798, 197.98  , 196.0002],\n",
       "       [197.98  , 199.9798, 197.98  , 196.0002, 194.0402],\n",
       "       [196.0002, 197.98  , 196.0002, 194.0402, 192.0998],\n",
       "       [194.0402, 196.0002, 194.0402, 192.0998, 190.1788]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Policy Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Policy-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2     3     4\n",
       "0  right  up  left    up  left\n",
       "1     up  up    up  left  left\n",
       "2     up  up    up    up    up\n",
       "3     up  up    up    up    up\n",
       "4     up  up    up    up    up"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

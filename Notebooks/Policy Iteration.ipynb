{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "discount_factor = 0.99 # small prefer immediate reward, large prefer future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a random policy\n",
    "# random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "# random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "# policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 8.901144212813961e-07 iterations: 2462'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "        # save data for plot\n",
    "        delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201.9998, 204.0402, 201.9998, 199.0402, 197.0498],\n",
       "       [199.9798, 201.9998, 199.9798, 197.98  , 196.0002],\n",
       "       [197.98  , 199.9798, 197.98  , 196.0002, 194.0402],\n",
       "       [196.0002, 197.98  , 196.0002, 194.0402, 192.0998],\n",
       "       [194.0402, 196.0002, 194.0402, 192.0998, 190.1788]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2462"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(delta_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get every 25th value\n",
    "# delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list)\n",
    "plt.title('Policy Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Policy-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2     3     4\n",
       "0  right  up  left    up  left\n",
       "1     up  up    up  left  left\n",
       "2     up  up    up    up    up\n",
       "3     up  up    up    up    up\n",
       "4     up  up    up    up    up"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "discount_factor = 0.99 # small prefer immediate reward, large prefer future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 8.90943681497447e-07 iterations: 2734'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "            # save data for plot\n",
    "            delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201.9998, 204.0402, 201.9998, 199.0402, 197.0498],\n",
       "       [199.9798, 201.9998, 199.9798, 197.98  , 196.0002],\n",
       "       [197.98  , 199.9798, 197.98  , 196.0002, 194.0402],\n",
       "       [196.0002, 197.98  , 196.0002, 194.0402, 192.0998],\n",
       "       [194.0402, 196.0002, 194.0402, 192.0998, 190.1788]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Policy Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Policy-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2     3     4\n",
       "0  right  up  left    up  left\n",
       "1     up  up    up  left  left\n",
       "2     up  up    up    up    up\n",
       "3     up  up    up    up    up\n",
       "4     up  up    up    up    up"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "discount_factor = 0.99 # small prefer immediate reward, large prefer future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 8.902583772396611e-07 iterations: 2658'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "            # save data for plot\n",
    "            delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201.9998, 204.0402, 201.9998, 199.0402, 197.0498],\n",
       "       [199.9798, 201.9998, 199.9798, 197.98  , 196.0002],\n",
       "       [197.98  , 199.9798, 197.98  , 196.0002, 194.0402],\n",
       "       [196.0002, 197.98  , 196.0002, 194.0402, 192.0998],\n",
       "       [194.0402, 196.0002, 194.0402, 192.0998, 190.1788]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Policy Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Policy-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2     3     4\n",
       "0  right  up  left    up  left\n",
       "1     up  up    up  left  left\n",
       "2     up  up    up    up    up\n",
       "3     up  up    up    up    up\n",
       "4     up  up    up    up    up"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

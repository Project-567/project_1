{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "discount_factor = 0.8 # small prefer immediate reward, large prefer future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 6.516798087830011e-07 iterations: 116'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "            # save data for plot\n",
    "            delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.8991, 14.8739, 11.8991, 10.2459,  8.1967],\n",
       "       [ 9.5193, 11.8991,  9.5193,  8.1967,  6.5574],\n",
       "       [ 7.6154,  9.5193,  7.6154,  6.5574,  5.2459],\n",
       "       [ 6.0923,  7.6154,  6.0923,  5.2459,  4.1967],\n",
       "       [ 4.8739,  6.0923,  4.8739,  4.1967,  3.3574]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Policy Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Policy-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2   3     4\n",
       "0  right  up  left  up  left\n",
       "1     up  up    up  up    up\n",
       "2     up  up    up  up    up\n",
       "3     up  up    up  up    up\n",
       "4     up  up    up  up    up"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

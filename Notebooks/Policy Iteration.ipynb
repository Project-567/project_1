{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the value_iteration.py code to be policy_iteration\n",
    "\n",
    "# policy iteration\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states\n",
    "\n",
    "# iterations = 0\n",
    "theta = 0.000001\n",
    "discount_factor = 0.95\n",
    "delta_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(value_map, states, discount_factor, theta, reward, transition, trans_prob, policy):\n",
    "    iterations = 0\n",
    "    delta_list = []\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(value_map)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = transition(state, action)\n",
    "                rewards = reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*trans_prob*(rewards+(discount_factor*value_map[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - value_map[state[0], state[1]]))      \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "        # save data for plot\n",
    "        delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        value_map = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return value_map, iterations, delta_list, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THE FOLLOWING FOR EVEN POLICY\n",
    "# # initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# # for equal probability amongst all actions, divide everything by the number of actions\n",
    "# policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# Initiate a random policy\n",
    "random_policy = np.random.randint(1000, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy\n",
    "\n",
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 3.94057156069843 iterations: 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define some basic parameters\n",
    "final_max_iter = 0\n",
    "old_policy = np.zeros([state_count, action_count])\n",
    "final_value_map = grid.valueMap\n",
    "\n",
    "# POLICY ITERATION #####################################3\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "\n",
    "    # run policy evaluation\n",
    "    final_value_map, max_iter, delta, policy = policy_evaluation(final_value_map, grid.states, discount_factor, theta, grid.reward, \n",
    "                                                                    grid.p_transition, grid.transition_prob, policy)\n",
    "\n",
    "    # for plotting purpose\n",
    "    final_max_iter += max_iter\n",
    "    delta_list.extend(delta)\n",
    "  \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, final_value_map)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "\n",
    "    # check if policy changed\n",
    "    if np.array_equal(old_policy, policy):\n",
    "        policy_stopped_changing = final_max_iter - max_iter\n",
    "    old_policy = policy\n",
    "    \n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Table: \n",
      "       0   1     2     3     4\n",
      "0  right  up  left    up  left\n",
      "1     up  up    up  left  left\n",
      "2     up  up    up    up    up\n",
      "3     up  up    up    up    up\n",
      "4     up  up    up    up    up\n",
      "\n",
      "Value Map: \n",
      "[[41.99468772 44.20493459 41.99468772 39.20493459 37.24468772]\n",
      " [39.89495334 41.99468772 39.89495334 37.90020515 36.00519443]\n",
      " [37.90020515 39.89495334 37.90020515 36.00519443 34.20493459]\n",
      " [36.00519443 37.90020515 36.00519443 34.20493459 32.49468772]\n",
      " [34.20493459 36.00519443 34.20493459 32.49468772 30.86995334]]\n"
     ]
    }
   ],
   "source": [
    "# PRINT POLICY TABLE ################################################################################\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "\n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()\n",
    "print(\"Value Map: \")\n",
    "print(final_value_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT DELTA PLOT #####################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "# plot iteration vs delta\n",
    "plt.plot(range(final_max_iter), delta_list)\n",
    "plt.title('Policy Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/policy_iteration_'+str(int(discount_factor*100))+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0dd44ab6a0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(final_max_iter), delta_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

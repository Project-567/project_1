{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 8.424983333199521e-07 iterations: 74'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Value Iteration\n",
    "\n",
    "discount_factor = 0.8 # small prefer immediate reward, large prefer future reward\n",
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    iterations+=1\n",
    "    valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "    \n",
    "    # FIND OPTIMAL VALUE #######################################################   \n",
    "    # start with the first state in the state list\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "        value = 0\n",
    "\n",
    "        # calculate best action value given current state and value function\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # choose the best action value\n",
    "        best_action_value = np.max(action_values)\n",
    "\n",
    "        # value of current state is equal to the best action value\n",
    "        value = best_action_value\n",
    "\n",
    "        # replace the value in valueMap with the value\n",
    "        valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "        # calculate delta\n",
    "        delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "        clear_output(wait=True)\n",
    "        display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "        # save data for plot\n",
    "        delta_list.append(delta)\n",
    "\n",
    "    # overwrite the original value map (after complete iteration of every step)\n",
    "    grid.valueMap = valueMap_copy\n",
    "\n",
    "    # stop when change in value function falls below a given threshold\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# EXTRACT POLICY FROM OPTIMAL VALUE #####################################################\n",
    "for state_number, state in enumerate(grid.states):\n",
    "    # using the current value map (optimal at this point), calculate the action values\n",
    "    action_values = calculate_action_value(state, grid.valueMap)\n",
    "    # return the action with the highest action value\n",
    "    best_action = np.argmax(action_values)\n",
    "    # update policy accordingly\n",
    "    policy[state_number] = np.eye(action_count)[best_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.8991, 14.8739, 11.8991, 10.2459,  8.1967],\n",
       "       [ 9.5193, 11.8991,  9.5193,  8.1967,  6.5574],\n",
       "       [ 7.6154,  9.5193,  7.6154,  6.5574,  5.2459],\n",
       "       [ 6.0923,  7.6154,  6.0923,  5.2459,  4.1967],\n",
       "       [ 4.8739,  6.0923,  4.8739,  4.1967,  3.3574]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Value Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Value-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2   3     4\n",
       "0  right  up  left  up  left\n",
       "1     up  up    up  up    up\n",
       "2     up  up    up  up    up\n",
       "3     up  up    up  up    up\n",
       "4     up  up    up  up    up"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Value Iteration\n",
    "\n",
    "# iterations = 0\n",
    "# delta_list = []\n",
    "\n",
    "# while True:\n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "\n",
    "#         # calculate best action value given current state and value function\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#         # choose the best action value\n",
    "#         best_action_value = np.max(action_values)\n",
    "\n",
    "#         # value of current state is equal to the best action value\n",
    "#         value = best_action_value\n",
    "\n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "#         # save data for plot\n",
    "#         delta_list.append(delta)\n",
    "\n",
    "#     # overwrite the original value map\n",
    "#     grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # turn optimal value in to action and update policy\n",
    "# for state_number, state in enumerate(grid.states):\n",
    "#     action_values = calculate_action_value(state, grid.valueMap)\n",
    "#     best_action = np.argmax(action_values)\n",
    "#     policy[state_number] = np.eye(action_count)[best_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

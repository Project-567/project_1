{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 1.2317230186909e-06 iterations: 1584'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "\n",
    "discount_factor = 0.99 # small prefer immediate reward, large prefer future reward\n",
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    iterations+=1\n",
    "    valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "    \n",
    "    # FIND OPTIMAL VALUE #######################################################   \n",
    "    # start with the first state in the state list\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "        value = 0\n",
    "\n",
    "        # calculate best action value given current state and value function\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # choose the best action value\n",
    "        best_action_value = np.max(action_values)\n",
    "\n",
    "        # value of current state is equal to the best action value\n",
    "        value = best_action_value\n",
    "\n",
    "        # replace the value in valueMap with the value\n",
    "        valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "        # calculate delta\n",
    "        delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "        clear_output(wait=True)\n",
    "        display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "        # save data for plot\n",
    "        delta_list.append(delta)\n",
    "\n",
    "    # overwrite the original value map (after complete iteration of every step)\n",
    "    grid.valueMap = valueMap_copy\n",
    "\n",
    "    # stop when change in value function falls below a given threshold\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# EXTRACT POLICY FROM OPTIMAL VALUE #####################################################\n",
    "for state_number, state in enumerate(grid.states):\n",
    "    # using the current value map (optimal at this point), calculate the action values\n",
    "    action_values = calculate_action_value(state, grid.valueMap)\n",
    "    # return the action with the highest action value\n",
    "    best_action = np.argmax(action_values)\n",
    "    # update policy accordingly\n",
    "    policy[state_number] = np.eye(action_count)[best_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201.9998, 204.0402, 201.9998, 199.0402, 197.0498],\n",
       "       [199.9798, 201.9998, 199.9798, 197.98  , 196.0002],\n",
       "       [197.98  , 199.9798, 197.98  , 196.0002, 194.0402],\n",
       "       [196.0002, 197.98  , 196.0002, 194.0402, 192.0998],\n",
       "       [194.0402, 196.0002, 194.0402, 192.0998, 190.1788]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Value Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Value-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2     3     4\n",
       "0  right  up  left    up  left\n",
       "1     up  up    up  left  left\n",
       "2     up  up    up    up    up\n",
       "3     up  up    up    up    up\n",
       "4     up  up    up    up    up"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Value Iteration\n",
    "\n",
    "# iterations = 0\n",
    "# delta_list = []\n",
    "\n",
    "# while True:\n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "\n",
    "#         # calculate best action value given current state and value function\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#         # choose the best action value\n",
    "#         best_action_value = np.max(action_values)\n",
    "\n",
    "#         # value of current state is equal to the best action value\n",
    "#         value = best_action_value\n",
    "\n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "#         # save data for plot\n",
    "#         delta_list.append(delta)\n",
    "\n",
    "#     # overwrite the original value map\n",
    "#     grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # turn optimal value in to action and update policy\n",
    "# for state_number, state in enumerate(grid.states):\n",
    "#     action_values = calculate_action_value(state, grid.valueMap)\n",
    "#     best_action = np.argmax(action_values)\n",
    "#     policy[state_number] = np.eye(action_count)[best_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

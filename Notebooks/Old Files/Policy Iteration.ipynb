{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []\n",
    "discount_factor = 0.99 # small prefer immediate reward, large prefer future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "            # save data for plot\n",
    "            delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201.9998, 204.0402, 201.9998, 199.0402, 197.0498],\n",
       "       [199.9798, 201.9998, 199.9798, 197.98  , 196.0002],\n",
       "       [197.98  , 199.9798, 197.98  , 196.0002, 194.0402],\n",
       "       [196.0002, 197.98  , 196.0002, 194.0402, 192.0998],\n",
       "       [194.0402, 196.0002, 194.0402, 192.0998, 190.1788]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Policy Iteration with Discount Factor ' + str(discount_factor))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')\n",
    "plt.savefig('graphs/Policy-'+str(discount_factor)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2     3     4\n",
       "0  right  up  left    up  left\n",
       "1     up  up    up  left  left\n",
       "2     up  up    up    up    up\n",
       "3     up  up    up    up    up\n",
       "4     up  up    up    up    up"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

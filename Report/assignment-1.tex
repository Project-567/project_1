\documentclass[11pt]{article}
\usepackage{graphicx}
\graphicspath{ {./pictures/} }

\usepackage[hoptionsi]{subcaption}

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Dynamic Distributed Decision Making \\Project 1 \\MIE567} % Title of the assignment

\author{\texttt{Hao Tan 999735728}\\ \texttt{Xiali Wu 999011322} \\ \texttt{David Molina 1005615318}} % Author name and email address

\date{University of Toronto --- \today} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

\section{Modelling}
\textbf{First, you are tasked with modelling the Gridworld domain
above as a Markov decision process. Then, you are asked to provide a complete
programming description of the problem that will be used to solve it
computationally.}
\\

% 1---------------------------------------------------------------
\noindent
\textbf{1}
\noindent
\textbf{Explain how you would model this navigation problem as a Markov
decision process. In particular:}
\\

% 1A---------------------------------------------------------------
\noindent
\textbf{a)}
\noindent
\textbf{Why is this problem an MDP? }
\\

\noindent
This problem can be described as an MDP because it can be modeled in terms of
components such as a reward function, actions, transition probability, states,
time steps and discount factor. Additionally, this problem is an infinite
horizon MDP since we can take unlimited amounts of steps between cells, and
possibly re-visit the same cell. In this case, unlimited amount of steps can
potentially happen to maximize rewards; therefore, a discount factor is included
for future rewards.
\\

\noindent
Furthermore, based on the definition of Markovian property, a stochastic process
has the Markov property if the conditional probability distribution of future
states of the process (conditional on both past and present states) depends only
upon the present state, not on the sequence of events that preceded it (ie.
P[St+1 | St] = P[St+1 | S1, S2 ... St]). In the given problem description, the
agent can only move one cell per action, while the next cell is only dependent
on the current cell. In other words, the next cell is not dependent on previous
cells before the current cell.
\\

% 1B---------------------------------------------------------------
\noindent
\textbf{b)}
\noindent
\textbf{What are suitable state and action spaces for this problem? Are
these the only possible choices? Why or why not?}
\\

\noindent
The states can be represented as the coordinates of the grid world (i,j), where i=
0,..,4, and j=0,...,4, as seen in the following.

\begin{equation}
\text { states }=\left[\begin{array}{lllll}
{(0,0)} & {(0,1)} & {(0,2)} & {(0,3)} & {(0,4)} \\
{(1,0)} & {(1,1)} & {(1,2)} & {(1,3)} & {(1,4)} \\
{(2,0)} & {(2,1)} & {(2,2)} & {(2,3)} & {(2,4)} \\
{(3,0)} & {(3,1)} & {(3,2)} & {(3,3)} & {(3,4)} \\
{(4,0)} & {(4,1)} & {(4,2)} & {(4,3)} & {(4,4)}
\end{array}\right]
\end{equation}


\noindent
The available set of actions are North, East, West, South, which can be implemented
using the addition and subtraction of grid world coordinates. For example, the following
shows the possible actions where (-1, 0) means North, (0,1) means East, (1,0) means South,
and (0,-1) means West.

\begin{equation}
\text { actions }=[(-1,0),(0,1),(1,0),(0,-1)]
\end{equation}

\noindent
There are other alternatives for the representation of this problem, for
example, each cells can be represented with a number from 1 to 25 and actions
can be represented as letters A = {N, E, W, S} or {up, right, down, left}. These
type of representations are tidieous to implement. For example, using continuous
numbers to represent states would be difficult to stop the agent from moving
off-grid without excessive \textit{if-statements}. In terms of other possible
actions, one may considerd diagonal actions such as those shown below.

\begin{equation}
\text { diagonal actions }=[(1,1),(-1,1),(1,-1),(-1,-1)]
\end{equation}

\noindent
Diagonal actions require the agent to move two steps per action to arrive at the
intended cell. For example, to move North-West, the agent must move West then
Easy or vice versa. This conflicts with the problem description where the agent
is only allowed to move one cell at a time; therefore, this alternative  not
used in the model. \\


% 1C---------------------------------------------------------------
\noindent
\textbf{c)}
\noindent
\textbf{What is the transition probability matrix P? (You may describe just
the non-zero entries.)}
\\

\noindent
In a transition probability matrix, the sum of each row is equal to one. In the
transition probability matrix presented in the Appendix, the vertical axis along
the left represents the current state, $s$, while the horizontal axis along the
top represents the potential next state, $s'$. Assuming a random policy with
equal probability distribution for all actions, there is a 25\% chance of
arriving at the intended neighbor cell except for the cells along the border of
the grid. For the cells at each of the corners, there are 2 actions each that
would take the agent off grid. In this scenario, the problem description states
that the agent would remain in its current position; therefore, the corner
states have a 50\% chance of transitioning to its own state and 25\% chance of
transitioning to a neighbor state. The same logic can be applied for the other
border states, where there is always a probability of returning to its own state
if an off-grid action is taken. Additionally, there are 2 special states;
namely, State A and B where all actions would take the agent directly to A' and
B', respectively. The transition probability matrix is presented in the
\textbf{Appendix} \\

% 1D---------------------------------------------------------------
\noindent
\textbf{d)}
\noindent
\textbf{Is the reward function provided the only possible one? If so, explain
why. If not, provide an example of a different reward function that would lead
to the same optimal behaviour}
\\

\noindent
The reward function privided is by the problem is shown below. In this case, a
transition from A to A' and B to B' will yield a reward of 10 and 5,
respectively. A transition that would take the agent off grid would result in a
reward of -1 while all other transitions yield a reward of zero. 

\begin{equation}
\text { Reward }={[0,-1,+5,+10]}
\end{equation}

\noindent
Using the state and action representation described in previous questions, an
off grid action is detected when elements within the next state coordinate holds
a value greater or less than the column/row size. For example, the following
show two cases of an agent going off grid. In equation (5), the agent is
originally in state (0,0) and the chosen action is to the left/West (0, -1),
this results in a coordinate (0, -1) that featured an element smaller than the
grid column/row size. Conversely, in equation (6), the agent took an action
going to the right when in state (0, 4) which resulted in the next state, (0,
5), with an element larger than the row/column size. In these cases, the agent
receives a reward of 1.

\noindent
By having a different reward function, the overall optimization problem is different.
As a result, this is the only reward function that leads to optimal
behaviour as described in the problem description.

\begin{equation}
\left[\begin{array}{c}
{0} \\
{0}
\end{array}\right]+\left[\begin{array}{c}
{0} \\
{-1}
\end{array}\right]=\left[\begin{array}{c}
{0} \\
{-1}
\end{array}\right]
\end{equation}

\begin{equation}
\left[\begin{array}{l}
{0} \\
{4}
\end{array}\right]+\left[\begin{array}{l}
{0} \\
{1}
\end{array}\right]=\left[\begin{array}{l}
{0} \\
{5}
\end{array}\right]
\end{equation}
\\

% 1E---------------------------------------------------------------
\noindent
\textbf{e)}
\noindent
\textbf{Derive the discounted Bellman equation for the problem, and simplify
as much as you can. (Hint: to avoid deriving a separate value for each state,
try to find groups of states such that you can write a single expression for V for
them) What do you think is/are the optimal policy/policies for this problem,
and why (you do NOT need to solve the Bellman equations)?}
\\

% \noindent
% Since the goal is maximizing reward, optimal policy always looks for A or B to
% get +10 or +5 reward. If the initial state is away from row 0 where A and B are
% located, such as row 4, the optimal policy requires more actions to get to the
% vicinity of A or B. If the initial state is in the vicinity of A and B, there is
% a tradeoff between more actions towards A and earning discounted +5 more reward
% from A->A’ compared to B->B’.
% \\

\noindent
To write the Bellman equations for this problem, we have grouped together states with similar
value functions. The different groups are shown in color in the following figure.

\begin{figure}[h]
\includegraphics[scale=0.5]{bellman_groups}
\centering
\caption{Groups of Bellman Equations}
\end{figure}

\noindent
The following shows the Bellman equation.

\begin{equation}
v_{\pi}(s)=\sum_{a} \pi(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]
\end{equation}


% 1E BELLMAN EQUATIONS -----------------------------------------------------
\noindent
The following represents the value function for state (0, 0)

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i, j))(-1+\gamma v(i, j))+P([-1,0] |(i, j))(-1+\gamma v(i, j))} \\
{+P([0,+1](i, j))(0+\gamma v(i, j+1))+P([+1, 0] |(i, j))(0+\gamma v(i+1, j))} \\
{\quad \text { for } i \in\{0\}, j \in\{0\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (0, 1)

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i, j))(10+\gamma v(i+4, j))+P([-1,0]|(i,j))(10+\gamma v(i+4, j))} \\
{+P([0,+1] |(i, j))(10+\gamma v(i+4, j))+P([+1,0]|(i,j))(10+\gamma v(i+4, j))} \\
{\quad \text { for } i \in\{0\}, j \in\{1\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (0, 2)

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i, j))(0+\gamma v(i, j-1))+P([-1,0] |(i, j))(-1+\gamma v(i, j))} \\
{+P([0,+1] |(i, j))(0+\gamma v(i, j+1))+P([+1,0] |(i, j))(0+\gamma v(i+1, j))} \\
{\quad \text { for } i \in\{0\}, j \in\{2\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (0, 3)

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i, j))(5+\gamma v(i+2, j))+P([-1,0] |(i, j))(5+\gamma v(i+2, j))} \\
{+P([0,+1] |(i, j))(5+\gamma v(i+2, j))+P([+1,0] |(i, j))(5+\gamma v(i+2, j))} \\
{\quad \text { for } i \in\{0\}, j \in\{3\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (0, 4)

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i, j))(0+\gamma v(i, j-1))+P([-1,0] |(i, j))(-1+\gamma v(i, j))} \\
{+P([0,+1] |(i, j))(-1+\gamma v(i, j))+P([+1,0] |(i, j))(0+\gamma v(i+1, j))} \\
{\quad \text { for } i \in\{0\}, j \in\{4\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (1,0), (2,0), (3,0).

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i, j))(-1+\gamma v(i, j))+P([-1,0] |(i, j))(0+\gamma v(i-1, j))}\\
{+P([0,+1] |(i, j))(0+\gamma(i, j+1))+P([+1,0] |(i, j))(0+\gamma v(i+1, j))}\\
{\text { for } i \in\{1,2,3\}, j \in\{0\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (1,1), (1,2), (1,3),
(2,1),(2,2),(2,3),\\
(3,1),(3,2),(3,3).

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i, j))(0+\gamma v(i, j-1))+P([-1,0] |(i, j))(0+\gamma v(i-1, j))} \\
{+P([0,+1]|(i, j))( 0+\gamma v(i, j+1))+P([+1,0] |(i, j))(0+\gamma v(i+1, j))} \\
{\quad \text { for } i \in\{1,2,3\}, j \in\{1,2,3\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (1,4),(2,4),(3,4).

\begin{equation}
\begin{array}{c}
{P([0,-1]|(i, j))(0+\gamma v(i, j-1))+P([-1,0] |(i, j))(0+\gamma v(i-1, j))} \\
{+P([0,+1]|(i, j))(-1+\gamma v(i, j))+P([+1,0] |(i, j))(0+\gamma v(i+1, j))} \\
{ \text { for } i \in\{1,2,3\}, j \in\{4\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (4,0).

\begin{equation}
\begin{array}{c}
{P([0,-1]|(i, j))(-1+\gamma v(i, j))+P([-1,0]|(i, j))(0+\gamma v(i-1, j))} \\
{+P([0,+1]|(i, j))(0+\gamma v(i, j+1))+P([+1,0]|(i, j)(-1+\gamma v(i, j))} \\
{\quad \text{for } i \in\{4\}, j \in\{0\}}
\end{array}
\end{equation}

\noindent
The following represents the value function for state (4,1), (4,2), (4,3).

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i,j))(0+\gamma v(i, j-1))+P([-1,0] |(i, j))(0+\gamma v(i-1, j))} \\
{+P([0,+1] |(i, j))(0+\gamma v(i, j+1))+P([+1,0] |(i, j))(-1+\gamma v(i, j))}\\
{\quad \text { for } i \in\{4\}, j \in\{1,2,3\}}
\end{array}\\
\end{equation}

\noindent
The following represents the value function for state (4,4).

\begin{equation}
\begin{array}{c}
{P([0,-1] |(i, j))(0+\gamma v(i, j-1))+P([-1,0]|(i, j))(0+\gamma v(i-1, j))} \\
{+P([0,+1]|(i, j))(1+\gamma v(i, j))+P([+1,0]|(i,j))(-1+\gamma v(i, j))} \\
{\quad \text { for } i \in\{4\}, j \in\{4\}}
\end{array}
\end{equation}


% 2---------------------------------------------------------------
\newpage
\noindent
\textbf{2}
\noindent
\textbf{Now, in a Python file called Gridworld.py, create a class that replicates
the behaviour of the MDP you formulated in the previous question. Your class
should contain four functions: one to return the initial state of the MDP, one to
return a view of all possible states, and two to return, respectively, the reward and
probability of a transition (s; a; s') from state s to state s' when taking action a.}
\\

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Returns a random initial state}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
    def initial_state(self):
        # randomly generate an initial state
        i = random.randint(0, len(self.states)-1)
        rand_state = self.states[i]
        return rand_state
\end{lstlisting}

\lstset{caption={Returns a view of all possile states}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
    def possible_states(self):
        # return the possible states
        return self.states
\end{lstlisting}

\lstset{caption={Returns probability of transition from $s$ to $s'$ given $a$}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
    def transition_probability(self, current_pos, new_pos):
        # deterministic environment = s + a has a 100% probability of ending up in s'
        return 1
\end{lstlisting}

\lstset{caption={Returns a reward given current position and action }}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
    def reward(self, current_pos, action):
        # take action in current pos
        self.new_pos = np.array(current_pos) + np.array(action)
        # normally, reward = 0
        reward = 0
        # if new pos results in off the grid, return reward -1
        if -1 in self.new_pos or self.size in self.new_pos:
            reward = -1
        # if in state A, transition to state A'
        if current_pos == [0, 1]:
            reward = 10
        # if in state B, transition to state B'
        if current_pos == [0, 3]:
            reward = 5
        return reward
\end{lstlisting}


% POLICY EVALUATION---------------------------------------------------------------
\newpage
\section{Policy Evaluation}
\textbf{Now, suppose the agent selects all four actions with equal probability. Use your
answers to questions 1 and 2 to write a python function, in a new file policy
evaluation.py to find the value function for this policy. You may use either an
iterative method or solve the system of equations. Show the value function you
obtained to at least four decimals.}
\\

\noindent
Following value function is generated based on equal action probability, and  =
0.99. The value function found using an iterative method is as follow:
\\

\begin{figure}[h]
\includegraphics[scale=0.6]{v_evaluation}
\centering
\caption{Value Function using Policy Evaluation}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.7]{policy_evaluation_99}
\centering
\caption{Convergence of Policy Evaluation}
\end{figure}


% VALUE ITERATION---------------------------------------------------------------
\newpage
\section{Value Iteration}
\textbf{The goal now is to solve the MDP above using dynamic programming. In a separate
file called value iteration.py, provide a complete implementation of the value
iteration algorithm you learned in class for solving the Bellman equations you
derived earlier.}
\\

\noindent
\textbf{-}
\noindent
\textbf{Did your algorithm converge at all?}
\\

\noindent
\textbf{-}
\noindent
\textbf{How many iterations did this take for each value of $\gamma$ ?}
\\

\noindent
\textbf{-}
\noindent
\textbf{What is the best value of $\gamma$? Also, What was the final policy you
obtained for each value of $\gamma$?}
\\

\noindent
\textbf{-}
\noindent
\textbf{Does the optimal policy you obtained correspond to the policy you
conjectured earlier?}
\\


\begin{figure}[h]
\includegraphics[scale=0.48]{VI_chart}
\centering
\caption{Value Iteration results}
\end{figure}




% POLICY ITERATION---------------------------------------------------------------
\newpage
\section{Policy Iteration}
\textbf{Repeat the previous part of the assignment, but now implement the policy
iteration algorithm from class in a file called policy iteration.py. Report your
results and answer all questions as in the previous part.}
\\

\noindent
\textbf{-}
\noindent
\textbf{Did your algorithm converge at all?}
\\

\noindent
\textbf{-}
\noindent
\textbf{How many iterations did this take for each value of $\gamma$ ?}
\\

\noindent
\textbf{-}
\noindent
\textbf{What is the best value of $\gamma$? Also, What was the final policy you
obtained for each value of $\gamma$?}
\\

\noindent
\textbf{-}
\noindent
\textbf{Does the optimal policy you obtained correspond to the policy you
conjectured earlier?}
\\

\begin{figure}[h]
\includegraphics[scale=0.48]{PI_chart}
\centering
\caption{Policy Iteration results}
\end{figure}


% ALGORITHM COMPARISON---------------------------------------------------------------
\newpage
\section{Comparison of Algorithms}
\textbf{In the final section of your report, you must compare the two algorithms you
implemented and their performance on the Gridworld domain, and comment on any
differences you observed. In particular, please answer at least the following
questions in your report:}
\\

\noindent
\textbf{-}
\noindent
\textbf{Compare the performance (e.g. values) of the optimal policies obtained
using value and policy iteration to the performance of the policy that chooses
an action at random (as you analyzed earlier), and comment on the difference.}
\\

\noindent
\textbf{-}
\noindent
\textbf{Were the value functions and policies you obtained, and the number of
iterations required to obtain these policies, similar between algorithms? How do
your results differ for different values of the parameter(s) (e.g. $\gamma$)?}
\\

\noindent
\textbf{-}
\noindent
\textbf{Which algorithm was more difficult to implement and why? Which algorithm
do you think would work better if the problem was scaled up?}
\\

\noindent
\textbf{-}
\noindent
\textbf{Include any additional insights, challenges, 
or important observations that you discovered while building or
 running your experiments.}
\\


% APPENDIX-------------------------------------
\newpage
\section{Appendix}

\begin{figure}[h]
\includegraphics[scale=0.5]{transition_matrix}
\centering
\caption{Transition Matrix}
\end{figure}

\end{document}

\relax 
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Modelling}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Groups of Bellman equations\relax }}{3}}
\newlabel{lst:code_direct}{{1}{5}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Returns a random initial state}{5}}
\newlabel{lst:code_direct}{{2}{5}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Returns a view of all possile states}{5}}
\newlabel{lst:code_direct}{{3}{5}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}Returns a reward given current position and action}{5}}
\newlabel{lst:code_direct}{{4}{5}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}Returns the next state, $s'$, given $a$ in state $s$}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Policy Evaluation}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Value function using policy evaluation with discount factor 0.99\relax }}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Convergence of policy evaluation\relax }}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Value Iteration}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Value iteration results: delta function graph, total iterations, optimal policy\relax }}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Policy Iteration}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Policy iteration results: delta function, total iterations, optimal policy\relax }}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison of Algorithms}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Value functions of policy evaluation, value iteration and policy iteration\relax }}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Transition Matrix\relax }}{13}}

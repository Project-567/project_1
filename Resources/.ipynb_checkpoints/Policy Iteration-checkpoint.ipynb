{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.95 # small prefer immediate reward, large prefer future reward\n",
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 2.66950076621697e-06 iterations: 190'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "            # save data for plot\n",
    "            delta_list.append(delta)\n",
    "            \n",
    "        # overwrite the original value map (after a full iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41.9947, 44.2049, 41.9947, 39.2049, 37.2447],\n",
       "       [39.895 , 41.9947, 39.895 , 37.9002, 36.0052],\n",
       "       [37.9002, 39.895 , 37.9002, 36.0052, 34.2049],\n",
       "       [36.0052, 37.9002, 36.0052, 34.2049, 32.4947],\n",
       "       [34.2049, 36.0052, 34.2049, 32.4947, 30.87  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Max Delta')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Policy Iteration with Discount Factor 0.99')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2     3     4\n",
       "0  right  up  left    up  left\n",
       "1  right  up    up  left  left\n",
       "2  right  up    up    up    up\n",
       "3  right  up    up    up    up\n",
       "4  right  up    up    up    up"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

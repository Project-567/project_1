{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 1\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]] #row +/- 1 or column +/- 1\n",
    "max_iters = 1000\n",
    "gridSize = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        \n",
    "    def reward(self, current_pos, action):\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.pos_check or self.size in self.pos_check: \n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Gridworld(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[-0.5  10.   -0.25  5.   -0.5 ]\n",
      " [-0.25  0.    0.    0.   -0.25]\n",
      " [-0.25  0.    0.    0.   -0.25]\n",
      " [-0.25  0.    0.    0.   -0.25]\n",
      " [-0.5  -0.25 -0.25 -0.25 -0.5 ]]\n",
      "\n",
      "Iteration 2\n",
      "[[ 1.6875  9.75    3.4375  5.      0.4375]\n",
      " [-0.5     2.4375 -0.0625  1.1875 -0.5   ]\n",
      " [-0.4375 -0.0625  0.     -0.0625 -0.4375]\n",
      " [-0.5    -0.125  -0.0625 -0.125  -0.5   ]\n",
      " [-0.875  -0.5    -0.4375 -0.5    -0.875 ]]\n",
      "\n",
      "Iteration 3\n",
      "[[ 2.6562  9.5     4.2812  4.9375  0.8438]\n",
      " [ 0.5469  2.2812  1.7656  1.0938 -0.0781]\n",
      " [-0.625   0.4688 -0.0625  0.1562 -0.625 ]\n",
      " [-0.7344 -0.2812 -0.1719 -0.2812 -0.7344]\n",
      " [-1.1875 -0.7344 -0.625  -0.7344 -1.1875]]\n",
      "\n",
      "Iteration 10\n",
      "[[ 4.3514  8.351   5.3083  5.5598  2.0662]\n",
      " [ 2.4122  3.7326  3.0872  2.4848  0.9595]\n",
      " [ 0.4105  1.1042  1.0883  0.5556 -0.3513]\n",
      " [-1.1554 -0.5113 -0.457  -0.745  -1.4931]\n",
      " [-2.3268 -1.7418 -1.5663 -1.8444 -2.483 ]]\n",
      "\n",
      "Iteration 100\n",
      "[[ 3.3264  7.0172  4.4173  4.8326  1.703 ]\n",
      " [ 1.5906  2.8216  2.3572  1.8962  0.5286]\n",
      " [-0.4212  0.2766  0.2489 -0.1786 -1.0584]\n",
      " [-2.1756 -1.5881 -1.5042 -1.846  -2.5701]\n",
      " [-3.5624 -2.994  -2.8765 -3.1761 -3.8507]]\n",
      "\n",
      "Iteration 1000\n",
      "[[ -6.7648  -3.074   -5.6739  -5.2586  -8.3882]\n",
      " [ -8.5007  -7.2696  -7.734   -8.1951  -9.5627]\n",
      " [-10.5125  -9.8147  -9.8423 -10.2698 -11.1497]\n",
      " [-12.2668 -11.6793 -11.5954 -11.9372 -12.6613]\n",
      " [-13.6536 -13.0852 -12.9676 -13.2672 -13.9418]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# policy evaluation\n",
    "for it in range(max_iters):\n",
    "    valueMap_copy = np.copy(grid.valueMap)\n",
    "    # start with the first state in the state list\n",
    "    for state in grid.states:\n",
    "        weightedRewards = 0\n",
    "        # perform 4 actions per state and add the rewards (weightedRewards)\n",
    "        for action in actions:\n",
    "            # get next position and reward\n",
    "            new_position = grid.p_transition(state, action)\n",
    "            reward = grid.reward(state, action)\n",
    "            # calculate weighted rewards: 1/4[r + gamma * value(s')]\n",
    "            weightedRewards += (1/len(actions))*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))\n",
    "        # replace the value in valueMap with the weighted rewards\n",
    "        valueMap_copy[state[0], state[1]] = round(weightedRewards,4)\n",
    "    # overwrite the original value map\n",
    "    grid.valueMap = valueMap_copy\n",
    "    \n",
    "    # print value map\n",
    "    if it in [0,1,2,9, 99, 999, max_iters-1]:\n",
    "        print(\"Iteration {}\".format(it+1))\n",
    "        print(grid.valueMap)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get transition: starting in state [0, 0] with action [0, 1], the new state is [0, 1]\n",
    "state = [0, 0]\n",
    "action = [0, 1] # move right\n",
    "grid = Gridworld(5)\n",
    "new_position = grid.p_transition(state, action)\n",
    "new_position"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# display output\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.8 # small prefer immediate reward, large prefer future reward\n",
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a policy: create an array of dimension (number of states by number of actions)\n",
    "# for equal probability amongst all actions, divide everything by the number of actions\n",
    "policy = np.ones([state_count, action_count]) / action_count\n",
    "\n",
    "# policy at state 0 = [0, 0]\n",
    "# returns a probability for each action given state\n",
    "policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        \n",
    "        # take action in current pos\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return a random initial state\n",
    "# grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return all possible states\n",
    "# grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "theta = 0.000001\n",
    "delta_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate action value with the current policy\n",
    "\n",
    "def calculate_action_value(state, value):\n",
    "    A = np.zeros(action_count)\n",
    "    \n",
    "    # perform 4 actions per state and add the rewards (value)\n",
    "    for action_number, action in enumerate(actions):\n",
    "            \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "        \n",
    "        # get next position and reward\n",
    "        new_position = grid.p_transition(state, action)\n",
    "        reward = grid.reward(state, action)\n",
    "\n",
    "        # calculate value of action: transition_prob*[r + gamma * value(s')]\n",
    "        A[action_number] += grid.transition_prob*(reward+(discount_factor*value[new_position[0], new_position[1]]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delta: 6.516798087830011e-07 iterations: 116'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # POLICY EVALUATION ####################################\n",
    "        # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "        # to calculate the value of each action.\n",
    "        # Replace the value map with the calculated value.\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations+=1\n",
    "        valueMap_copy = np.copy(grid.valueMap)\n",
    "\n",
    "        # start with the first state in the state list\n",
    "        for state_number, state in enumerate(grid.states):\n",
    "            value = 0\n",
    "\n",
    "            # perform 4 actions per state and add the rewards (value)\n",
    "            for action_number, action in enumerate(actions):\n",
    "\n",
    "                # get next position and reward\n",
    "                new_position = grid.p_transition(state, action)\n",
    "                reward = grid.reward(state, action)\n",
    "\n",
    "                # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "                value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "\n",
    "            # replace the value in valueMap with the value\n",
    "            valueMap_copy[state[0], state[1]] = value\n",
    "\n",
    "            # calculate delta\n",
    "            delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "            clear_output(wait=True)\n",
    "            display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "\n",
    "            # save data for plot\n",
    "            delta_list.append(delta)\n",
    "\n",
    "        # overwrite the original value map (update valuemap after one complete iteration of every state)\n",
    "        grid.valueMap = valueMap_copy\n",
    "\n",
    "        # stop when change in value function falls below a given threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # POLICY IMPROVEMENT #######################################\n",
    "        # iterate through every state and choose the best action with the current policy\n",
    "        # calculate the action values of every state\n",
    "        # take the best action and compare whether the best action is the same as the chosen one\n",
    "        # update the policy with the best action\n",
    "    \n",
    "    # initate policy_true as stable\n",
    "    policy_stable = True\n",
    "\n",
    "    # iterate over every state\n",
    "    for state_number, state in enumerate(grid.states):\n",
    "\n",
    "        # choose the best action with the current policy\n",
    "        choose_action = np.argmax(policy[state_number])\n",
    "\n",
    "        # calculate the action values for each state using the current value function\n",
    "        # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "        action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "        # using the calculated action values, find the best action\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # if the chosen action is different than the calculated best action\n",
    "        # then the current policy is not stable\n",
    "        if choose_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # update the current policy with the new best action\n",
    "        policy[state_number] = np.eye(action_count)[best_action]\n",
    "\n",
    "    # if the policy is stable (eg. chosen action is the same as best action)\n",
    "    # then we can exit\n",
    "    # however, if it is not, then we need to perform policy evaluation and improvement again\n",
    "    if policy_stable:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.8991, 14.8739, 11.8991, 10.2459,  8.1967],\n",
       "       [ 9.5193, 11.8991,  9.5193,  8.1967,  6.5574],\n",
       "       [ 7.6154,  9.5193,  7.6154,  6.5574,  5.2459],\n",
       "       [ 6.0923,  7.6154,  6.0923,  5.2459,  4.1967],\n",
       "       [ 4.8739,  6.0923,  4.8739,  4.1967,  3.3574]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value map to 4 decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 25th value\n",
    "delta_list2 = delta_list[0::state_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Max Delta')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot iteration vs delta\n",
    "plt.plot(range(iterations), delta_list2)\n",
    "plt.title('Policy Iteration with Discount Factor 0.99')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Max Delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Policy Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    for action in range(policy.shape[1]):\n",
    "        if policy[state][action] == 1:\n",
    "            \n",
    "            # calculate the row and column coordinate of the current state number\n",
    "            row = int(state/grid.size)\n",
    "            column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "\n",
    "            # get action name\n",
    "            if action == 0:\n",
    "                action_name = 'up'\n",
    "            elif action == 1:\n",
    "                action_name = 'right'\n",
    "            elif action == 2:\n",
    "                action_name = 'down'\n",
    "            else:\n",
    "                action_name = 'left'\n",
    "            \n",
    "            # assign action name\n",
    "            policy_table.loc[row][column] = action_name\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1     2   3     4\n",
       "0  right  up  left  up  left\n",
       "1     up  up    up  up    up\n",
       "2     up  up    up  up    up\n",
       "3     up  up    up  up    up\n",
       "4     up  up    up  up    up"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print policy table\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy evaluation\n",
    "#     # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "#     # to calculate the value of each action.\n",
    "#     # Replace the value map with the calculated value.\n",
    "\n",
    "# theta = 0.001\n",
    "# iterations = 0\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     delta = 0\n",
    "#     iterations+=1\n",
    "#     valueMap_copy = np.copy(grid.valueMap)\n",
    "    \n",
    "#     # start with the first state in the state list\n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "#         value = 0\n",
    "        \n",
    "#         # perform 4 actions per state and add the rewards (value)\n",
    "#         for action_number, action in enumerate(actions):\n",
    "            \n",
    "#             # get next position and reward\n",
    "#             new_position = grid.p_transition(state, action)\n",
    "#             reward = grid.reward(state, action)\n",
    "            \n",
    "#             # calculate value: policy*transition_prob*[r + gamma * value(s')]\n",
    "#             value += policy[state_number][action_number]*grid.transition_prob*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))          \n",
    "            \n",
    "#         # replace the value in valueMap with the value\n",
    "#         valueMap_copy[state[0], state[1]] = value\n",
    "        \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, np.abs(value - grid.valueMap[state[0], state[1]]))       \n",
    "#         clear_output(wait=True)\n",
    "#         display('delta: ' + str(delta) + ' iterations: ' + str(iterations))\n",
    "        \n",
    "#         # overwrite the original value map\n",
    "#         grid.valueMap = valueMap_copy\n",
    "\n",
    "#     # stop when change in value function falls below a given threshold\n",
    "#     if delta < theta:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print value map to 4 decimal places\n",
    "# np.set_printoptions(precision=4)\n",
    "# grid.valueMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy improvement\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     policy_stable = True\n",
    "    \n",
    "#     for state_number, state in enumerate(grid.states):\n",
    "\n",
    "#          # The best action we would take under the current policy\n",
    "#         chosen_a = np.argmax(policy[state_number])\n",
    "\n",
    "#          # eg. action_values = [#, #, #, #] = a value for each of the 4 actions\n",
    "#         action_values = calculate_action_value(state, grid.valueMap)\n",
    "\n",
    "#          # take the action with the highest value\n",
    "#         best_a = np.argmax(action_values)\n",
    "\n",
    "#          # Greedily update the policy\n",
    "#         if chosen_a != best_a:\n",
    "#             policy_stable = False\n",
    "\n",
    "#          # update the policy with the best action\n",
    "#         policy[state_number] = np.eye(action_count)[best_a]\n",
    "\n",
    "#      # If the policy is stable we've found an optimal policy. Return it\n",
    "#     if policy_stable:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

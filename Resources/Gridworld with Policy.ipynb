{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 1\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]] #row +/- 1 or column +/- 1\n",
    "max_iters = 1000\n",
    "gridSize = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.pos_check = [0, 0] # a copy of new position\n",
    "    \n",
    "    def initial_state(self):\n",
    "        # randomly generate an initial state\n",
    "        i = random.randint(0, len(self.states)-1)\n",
    "        rand_state = self.states[i]\n",
    "        return rand_state\n",
    "    \n",
    "    def possible_states(self):\n",
    "        # return the possible states\n",
    "        return self.states\n",
    "    \n",
    "    def reward(self, current_pos, action):\n",
    "        # return the reward        \n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.pos_check or self.size in self.pos_check: \n",
    "            reward = -1\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "        return reward\n",
    "    \n",
    "    # def transition_probability(self, current_pos, new_pos):\n",
    "        # a function that returns the entries of the transition probability matrix?\n",
    "        # eg. input current state, new state, output = 0.25...0.5...1 ... etc. ?\n",
    "    \n",
    "    def p_transition(self, current_pos, action):\n",
    "        # return the transition probability\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "        self.pos_check = self.new_pos # make a copy of new pos before being overwritten below\n",
    "        # if taking an action crosses the border = agent stays in same position\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "        return self.new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "grid.valueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return a random initial state\n",
    "grid.initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [0, 1],\n",
       " [0, 2],\n",
       " [0, 3],\n",
       " [0, 4],\n",
       " [1, 0],\n",
       " [1, 1],\n",
       " [1, 2],\n",
       " [1, 3],\n",
       " [1, 4],\n",
       " [2, 0],\n",
       " [2, 1],\n",
       " [2, 2],\n",
       " [2, 3],\n",
       " [2, 4],\n",
       " [3, 0],\n",
       " [3, 1],\n",
       " [3, 2],\n",
       " [3, 3],\n",
       " [3, 4],\n",
       " [4, 0],\n",
       " [4, 1],\n",
       " [4, 2],\n",
       " [4, 3],\n",
       " [4, 4]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return all possible states\n",
    "grid.possible_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[-0.5  10.   -0.25  5.   -0.5 ]\n",
      " [-0.25  0.    0.    0.   -0.25]\n",
      " [-0.25  0.    0.    0.   -0.25]\n",
      " [-0.25  0.    0.    0.   -0.25]\n",
      " [-0.5  -0.25 -0.25 -0.25 -0.5 ]]\n",
      "\n",
      "Iteration 2\n",
      "[[ 1.6875  9.75    3.4375  5.      0.4375]\n",
      " [-0.5     2.4375 -0.0625  1.1875 -0.5   ]\n",
      " [-0.4375 -0.0625  0.     -0.0625 -0.4375]\n",
      " [-0.5    -0.125  -0.0625 -0.125  -0.5   ]\n",
      " [-0.875  -0.5    -0.4375 -0.5    -0.875 ]]\n",
      "\n",
      "Iteration 3\n",
      "[[ 2.6562  9.5     4.2812  4.9375  0.8438]\n",
      " [ 0.5469  2.2812  1.7656  1.0938 -0.0781]\n",
      " [-0.625   0.4688 -0.0625  0.1562 -0.625 ]\n",
      " [-0.7344 -0.2812 -0.1719 -0.2812 -0.7344]\n",
      " [-1.1875 -0.7344 -0.625  -0.7344 -1.1875]]\n",
      "\n",
      "Iteration 10\n",
      "[[ 4.3514  8.351   5.3083  5.5598  2.0662]\n",
      " [ 2.4122  3.7326  3.0872  2.4848  0.9595]\n",
      " [ 0.4105  1.1042  1.0883  0.5556 -0.3513]\n",
      " [-1.1554 -0.5113 -0.457  -0.745  -1.4931]\n",
      " [-2.3268 -1.7418 -1.5663 -1.8444 -2.483 ]]\n",
      "\n",
      "Iteration 100\n",
      "[[ 3.3264  7.0172  4.4173  4.8326  1.703 ]\n",
      " [ 1.5906  2.8216  2.3572  1.8962  0.5286]\n",
      " [-0.4212  0.2766  0.2489 -0.1786 -1.0584]\n",
      " [-2.1756 -1.5881 -1.5042 -1.846  -2.5701]\n",
      " [-3.5624 -2.994  -2.8765 -3.1761 -3.8507]]\n",
      "\n",
      "Iteration 1000\n",
      "[[ -6.7648  -3.074   -5.6739  -5.2586  -8.3882]\n",
      " [ -8.5007  -7.2696  -7.734   -8.1951  -9.5627]\n",
      " [-10.5125  -9.8147  -9.8423 -10.2698 -11.1497]\n",
      " [-12.2668 -11.6793 -11.5954 -11.9372 -12.6613]\n",
      " [-13.6536 -13.0852 -12.9676 -13.2672 -13.9418]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# policy evaluation\n",
    "    # iterate through all 25 states. At each state, iterate through all 4 actions\n",
    "    # to calculate the value of each action.\n",
    "    # Replace the value map with the calculated value.\n",
    "\n",
    "for it in range(max_iters):\n",
    "    valueMap_copy = np.copy(grid.valueMap)\n",
    "    # start with the first state in the state list\n",
    "    for state in grid.states:\n",
    "        value = 0\n",
    "        # perform 4 actions per state and add the rewards (value)\n",
    "        for action in actions:\n",
    "            # get next position and reward\n",
    "            new_position = grid.p_transition(state, action)\n",
    "            reward = grid.reward(state, action)\n",
    "            # calculate value: 1/4[r + gamma * value(s')]\n",
    "            value += (1/len(actions))*(reward+(discount_factor*grid.valueMap[new_position[0], new_position[1]]))\n",
    "        # replace the value in valueMap with the value\n",
    "        valueMap_copy[state[0], state[1]] = round(value,4)\n",
    "    # overwrite the original value map\n",
    "    grid.valueMap = valueMap_copy\n",
    "    \n",
    "    # print value map\n",
    "    if it in [0,1,2,9, 99, 999, max_iters-1]:\n",
    "        print(\"Iteration {}\".format(it+1))\n",
    "        print(grid.valueMap)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get transition: starting in state [0, 0] with action [0, 1], the new state is [0, 1]\n",
    "state = [0, 0]\n",
    "action = [0, 1] # move right\n",
    "grid = Gridworld(5)\n",
    "new_position = grid.p_transition(state, action)\n",
    "new_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to solve a system of equations\n",
    "a = np.array([[0.5, 0, 0], [-0.5, 1, 0], [0, -0.5, 1]])\n",
    "b = np.array([0,0,1])\n",
    "x = np.linalg.solve(a,b)\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
